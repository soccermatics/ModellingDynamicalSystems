<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>A likely answer &mdash; Four Ways  documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/custom.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery-binder.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery-dataframe.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery-rendered-html.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Happy world" href="../gallery/lesson1/plot_howtobehappy.html" />
    <link rel="prev" title="Very average friends" href="averagefriends.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html">
            <img src="../_static/logo.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Statistical</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="averagefriends.html">Very average friends</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">A likely answer</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#the-data">The data</a></li>
<li class="toctree-l2"><a class="reference internal" href="#enter-ronald-fisher">Enter Ronald Fisher</a></li>
<li class="toctree-l2"><a class="reference internal" href="#likelihood-function">Likelihood function</a></li>
<li class="toctree-l2"><a class="reference internal" href="#logarithms-and-log-likelihood-function">Logarithms and log-likelihood function</a></li>
<li class="toctree-l2"><a class="reference internal" href="#the-maximum-likelihood">The Maximum Likelihood</a></li>
<li class="toctree-l2"><a class="reference internal" href="#fishers-original-paper">Fisher’s original paper</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../gallery/lesson1/plot_howtobehappy.html">Happy world</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gallery/lesson1/plot_happyperson.html">Happy individual</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Interactive</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../gallery/lesson2/plot_rabbitsandfoxes.html">Rabbits and foxes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gallery/lesson2/plot_socialepidemic.html">The Social Epidemic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gallery/lesson2/plot_morethanthesum.html">More than the sum of its parts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gallery/lesson2/plot_cellularautomata.html">Cellular Automata</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gallery/lesson2/plot_theartofagoodargument.html">The art of a good argument</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Chaotic</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../gallery/lesson3/plot_thebutterflyeffect.html">The butterfly effect</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gallery/lesson3/plot_cellularchaos.html">Cellular Chaos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gallery/lesson3/plot_informationequalsrandomness.html">Information Equals Randomness</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Complex</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../gallery/lesson4/plot_I_II_III_IV.html">I, II, III, IV</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gallery/lesson4/plot_all_of_the_life.html">All of the life</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Four Ways</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>A likely answer</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/lesson1/alikelyanswer.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="a-likely-answer">
<span id="alikelyanswer"></span><h1>A likely answer<a class="headerlink" href="#a-likely-answer" title="Permalink to this heading"></a></h1>
<p><strong>What we will learn:</strong> Understanding probabilities and likelihoods. Why
we measure proportions the way we do. Logarithms and log-likelihoods.
Find the maximum (log-)likelihood estimate and prove it is indeed
maximum. Understand some of the thought process behind Fisher’ original
paper <a class="reference external" href="https://www.jstor.org/stable/pdf/2246266.pdf?refreqid=excelsior%3Ace877822879bb8e9c1500ec9d6c0d244&amp;ab_segments=&amp;origin=&amp;acceptTC=1">On an Absolute Criterion for Fitting Frequency
Curves</a>.</p>
<p><strong>Pre-requisits:</strong> High school knowledge of <a class="reference external" href="https://www.khanacademy.org/math/cc-seventh-grade-math/cc-7th-probability-statistics#cc-7th-basic-prob">Basic
probability</a>
and
<a class="reference external" href="https://www.bbc.co.uk/bitesize/guides/zn3ty9q/revision/1">Logarithms</a>
will get you through the first half of this description. Later on we use
<a class="reference external" href="https://www.khanacademy.org/math/differential-calculus/dc-diff-intro#dc-product-rule">the product rule for
derivatives</a>
and <a class="reference external" href="https://www.khanacademy.org/math/in-in-grade-12-ncert/xd340c21e718214c5:continuity-differentiability/xd340c21e718214c5:logarithmic-functions-differentiation/v/logarithmic-functions-differentiation-intro">derivatives of
logarithms</a>.
Finally, fo more advanced students, in the final section we use
<a class="reference external" href="https://www.probabilitycourse.com">undergraduate probability and
statistics</a>.</p>
<div class="section" id="the-data">
<h2>The data<a class="headerlink" href="#the-data" title="Permalink to this heading"></a></h2>
<p>In the previous lesson we looked at the ‘Yes’ and ‘No’ answers to the
gherkin question as <span class="math notranslate nohighlight">\(1\)</span> for ‘Yes’ and <span class="math notranslate nohighlight">\(0\)</span> for ‘No’. This
gave us a table with a 1 if a person likes gherkins, a 0 if they don’t.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Ant
hony</p></th>
<th class="head"><p>A
isha</p></th>
<th class="head"><p>Cha
rlie</p></th>
<th class="head"><p>B
ecky</p></th>
<th class="head"><p>Jenn
ifer</p></th>
<th class="head"><p>Ric
hard</p></th>
<th class="head"><p>Nia</p></th>
<th class="head"><p>John</p></th>
<th class="head"><p>S
ofie</p></th>
<th class="head"><p>Suki</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
</tbody>
</table>
<p>Intuitively, it feels like the best estimate, from this data, of the
frequency of Millennial Londoners who like pickled gherkins is 4/10 or
40%. If we take the average of all the 1’s’ and 0’s in the table above,
we get exactly this answer:</p>
<div class="math notranslate nohighlight">
\[\frac{1+0+1+0+1+0+0+1+0+0}{10}=\frac{4}{10}\]</div>
<p>How do we know this is the correct answer? Imagine, for instance, that
some of the friends objected to using the average using some,
admittedly, quite dubious arguments. Antony might claim we should give
extra weighting to the answers of those you asked first because ‘they
are the originals’. He adds up <span class="math notranslate nohighlight">\(2 + 0 + 2 + 2 + 0 = 6\)</span> for the
first five and <span class="math notranslate nohighlight">\(0 + 0 + 1 + 0 + 0 = 1\)</span> for the last five, and
estimates the proportion be <span class="math notranslate nohighlight">\((6+ 1)/15 = 7/15\)</span>.</p>
<p>On hearing Antony’s argument, Aisha counter claims that it’s better to
ask 5 people and ignore all the others. She just looks at the answer of
every second person and finds that, out of this group, only one person
(John, as it turns out) likes pickled gherkins and concludes that the
proper proportion is <span class="math notranslate nohighlight">\(1/5\)</span>. Finally, Charlie says, ‘Hey guys.
Let’s just listen to the first person and accept what he says is true.
It will save us from arguments later on.’</p>
<p>Charlie proclaims that ‘Antony loves pickled gherkins, so everyone loves
pickled gherkins!’</p>
<p>How do we convince Antony, Aisha and Charlie that they are all wrong and
that there is only one correct way of measuring the proportion who like
pickled gherkins and it is 40%?</p>
</div>
<div class="section" id="enter-ronald-fisher">
<h2>Enter Ronald Fisher<a class="headerlink" href="#enter-ronald-fisher" title="Permalink to this heading"></a></h2>
<p>In <em>Four Ways</em> I take you back in time to visit Ronald Fisher and look
how he would have answered the question.</p>
<p>Fisher made the argument as follows. Imagine for now that we don’t know
the exact proportion of people that will answer ‘yes’ to the Gherkin
question– but we can be sure it has some value between zero and 100%. He
would then ask Antony (who suggested <span class="math notranslate nohighlight">\(7/15\)</span>), Aisha (who proposed
<span class="math notranslate nohighlight">\(1/5\)</span>), Charlie (who thinks 100% of people like gherkins) to
calculate the likelihood of their suggestions given the data on gherkin
preferences.</p>
<p>Let’s start with Aisha’s suggestion that the probability that a person
likes a gherkin is <span class="math notranslate nohighlight">\(1/5\)</span> or 20%. If she is correct then the
likelihood that we got the answer we got from Charlie is <span class="math notranslate nohighlight">\(1/5\)</span>,
since he said he liked gherkins. Similarly, again assuming, as Antony
does, that 80% of people don’t like gherkins, then the likelihood of
Sue’s answer is <span class="math notranslate nohighlight">\(4/5\)</span>. We can now write out a table of the
likelihood of each person’s answer, as follows,</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Ant
hony</p></th>
<th class="head"><p>A
isha</p></th>
<th class="head"><p>Cha
rlie</p></th>
<th class="head"><p>B
ecky</p></th>
<th class="head"><p>Jenn
ifer</p></th>
<th class="head"><p>Ric
hard</p></th>
<th class="head"><p>Nia</p></th>
<th class="head"><p>John</p></th>
<th class="head"><p>S
ofie</p></th>
<th class="head"><p>Suki</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1/5</p></td>
<td><p>4/5</p></td>
<td><p>1/5</p></td>
<td><p>4/5</p></td>
<td><p>1/5</p></td>
<td><p>4/5</p></td>
<td><p>4/5</p></td>
<td><p>1/5</p></td>
<td><p>4/5</p></td>
<td><p>4/5</p></td>
</tr>
</tbody>
</table>
<p>The combined likelihood of all the answers is found by multiplying the
likelihoods of all the answers together, i.e.</p>
<div class="math notranslate nohighlight">
\[1/5 \cdot 4/5 \cdot 1/5 \cdot 4/5 \cdot 1/5 \cdot 4/5 \cdot 4/5 \cdot 1/5 \cdot 4/5 \cdot 4/5=0.000419\]</div>
<p>where we use a dot (<span class="math notranslate nohighlight">\(\cdot\)</span>) to represent multiplication.</p>
<p>Clearly, the probability of this particular sequence of answers is very
small, because it is the probability of us getting a very specific
sequence of answers. This does not in itself prove that Aisha is wrong:
the probability of any sequence of answers is necessarily going to be
quite small. Instead, what is useful about this calculation is that it
allows us to compare the likelihood of Aisha’s proposal to the other
proposals.</p>
<p>To see how, let’s start by comparing the likelihood of Aisha’s estimate
to that of Charlie, who claimed that 100% of people liked gherkins. This
gives a likelihood of</p>
<div class="math notranslate nohighlight">
\[1 \cdot 0 \cdot 1 \cdot 0 \cdot 0 \cdot 0 \cdot 0 \cdot 1 \cdot 0 \cdot 0=0\]</div>
<p>There is literally zero likelihood of getting the answers we did given
his suggestion. He is proven wrong as soon as Aisha gives her answer.
So, Aisha wins that one. For Antony, we get,</p>
<div class="math notranslate nohighlight">
\[7/15 \cdot 8/15 \cdot 7/15 \cdot 8/15 \cdot 7/15 \cdot 8/15 \cdot 8/15 \cdot 7/15 \cdot 8/15 \cdot 8/15=0.00109\]</div>
<p>Antony is less wrong than Aisha, because 0.00109 is larger than
0.000419. But neither of them are as good as the correct estimate, of
4/10, for which we get a likelihood</p>
<div class="math notranslate nohighlight">
\[4/10 \cdot 6/10 \cdot 4/10 \cdot 6/10 \cdot 4/10 \cdot 6/10 \cdot 6/10 \cdot 4/10 \cdot 6/10 \cdot 6/10=0.00119\]</div>
<p>We have a winner! Our value of 40% has the largest likelihood out of
those we tested.</p>
</div>
<div class="section" id="likelihood-function">
<h2>Likelihood function<a class="headerlink" href="#likelihood-function" title="Permalink to this heading"></a></h2>
<p>In the above example we compare tried out different proportions and
calculated their likelihood. Let’s now the same exercise, but use the
letter <span class="math notranslate nohighlight">\(p\)</span> to denate the probability that a person likes gerkhins,
and the letter <span class="math notranslate nohighlight">\(l\)</span> to denate the likelihood of the answers. For
these particular answers,</p>
<div class="math notranslate nohighlight">
\[l = p \cdot (1-p) \cdot p \cdot (1-p) \cdot p \cdot (1-p) \cdot (1-p) \cdot p \cdot (1-p) \cdot (1-p)\]</div>
<p>where <span class="math notranslate nohighlight">\((1-p)\)</span> is the probability that a person doesn’t like
gherkins.</p>
<p>We can rewrite this equation using exponents, which are used to denote
multiplying numbers together. For example, if we multiply three twos
together (<span class="math notranslate nohighlight">\(2\cdot 2\cdot 2=8\)</span>), a short hand is to write
<span class="math notranslate nohighlight">\(2^3\)</span>. So, <span class="math notranslate nohighlight">\(2^3=2\cdot 2\cdot 2=8\)</span> and we say ‘<span class="math notranslate nohighlight">\(2\)</span> to
the power of <span class="math notranslate nohighlight">\(3\)</span> is <span class="math notranslate nohighlight">\(8\)</span>’. The number <span class="math notranslate nohighlight">\(3\)</span> is known as
the <em>exponent</em>. Similarly,
<span class="math notranslate nohighlight">\(2^6=2\cdot 2\cdot 2\cdot 2\cdot 2\cdot 2=64\)</span>. Now <span class="math notranslate nohighlight">\(6\)</span> is
the exponent. Multiplying numbers by themselves makes them very large,
very quickly. For example, <span class="math notranslate nohighlight">\(2^{20}=1,048,576\)</span> and
<span class="math notranslate nohighlight">\(2^{100}=1,267,650,600,228,229,401,496,703,205,376\)</span>. The number of
atoms in the Universe is (very) roughly equal to <span class="math notranslate nohighlight">\(2^{266}\)</span>.</p>
<p>Since it doesn’t matter which order we muliply, we can now write,</p>
<div class="math notranslate nohighlight">
\[l= p^4 \cdot (1-p)^6\]</div>
<p>The probability of the particular set of preferences expressed by these
10 people. When p=0.4 then this becomes l = (4/10)^4
4
〖〗<sup>6)/10</sup>10 =11664/9765625≈0.00119 as we
also saw above.</p>
<p>One way of showing that this is the best value is to plot the likelihood
for every possible value of <span class="math notranslate nohighlight">\(p\)</span>. This is done in the figure below:</p>
<img alt="../_images/likelihood.png" src="../_images/likelihood.png" />
<p>This has a maximum at <span class="math notranslate nohighlight">\(0.4\)</span>. This is why the estimate
<span class="math notranslate nohighlight">\(p=0.4\)</span> is the best estimate. If we look at any other value on
this plot, it is less likely than 0.4, which is the maximum.</p>
</div>
<div class="section" id="logarithms-and-log-likelihood-function">
<h2>Logarithms and log-likelihood function<a class="headerlink" href="#logarithms-and-log-likelihood-function" title="Permalink to this heading"></a></h2>
<p>When dealing with independent events, such as dice throws or coin tosses
or people liking gherkins, we multiply the probabilities of each event
in order to find the probability of them occurring. Just like repeatedly
multiplying by a number greater than 1 (such 2) makes them large very
quickly, multiplying probabilities makes them small very quickly. For
example, the probability of getting 10 sixes in a row is
<span class="math notranslate nohighlight">\((1/6)^{10}\)</span>, which is less than one in in 60 million. The fact
that multiplying makes numbers small (or large) very fast is one of the
reasons for using logarithms, which I will now introduce.</p>
<p>Logarithms are the opposite of powers. If we ask ‘what is
<span class="math notranslate nohighlight">\(\log_2(8)\)</span>?’ then we are asking how many times I need to multiply
<span class="math notranslate nohighlight">\(1\)</span> by <span class="math notranslate nohighlight">\(2\)</span> in order to get 8. The answer is that
<span class="math notranslate nohighlight">\(\log_2(8)\)</span>=3, since as we just saw, I need to multiply three
times to get 8 (i.e. <span class="math notranslate nohighlight">\(2^3=2\cdot 2\cdot 2=8\)</span>). Similarly,
<span class="math notranslate nohighlight">\(\log_2(64)=6\)</span>, since <span class="math notranslate nohighlight">\(2\)</span> multiplied <span class="math notranslate nohighlight">\(6\)</span> times is
<span class="math notranslate nohighlight">\(64\)</span>. The logarithm of <span class="math notranslate nohighlight">\(8\)</span> and <span class="math notranslate nohighlight">\(64\)</span> can be thus
thought of as undoing the power of <span class="math notranslate nohighlight">\(2\)</span> to give us <span class="math notranslate nohighlight">\(3\)</span> and
<span class="math notranslate nohighlight">\(6\)</span>, respectively.</p>
<p>The value <span class="math notranslate nohighlight">\(2\)</span> written in the subscript in <span class="math notranslate nohighlight">\(\log_2\)</span> is know
as the base of the logarithm. We can have other bases. So for example,
if I ask ‘what is <span class="math notranslate nohighlight">\(\log_{10}(10000)\)</span>?’ then I am asking how many
times I need to multiply <span class="math notranslate nohighlight">\(1\)</span> by <span class="math notranslate nohighlight">\(10\)</span> in order to get
<span class="math notranslate nohighlight">\(10,000\)</span>. The answer is _10(10000)=4.</p>
<p>Logarithms turn multiplication in to addition. For powers,
<span class="math notranslate nohighlight">\(2^3 \cdot 2^3= 2 \cdot 2 \cdot 2 \cdot 2 \cdot 2 \cdot 2 = 2^6\)</span>.
We add the exponents when we multiply. For logarithms,</p>
<div class="math notranslate nohighlight">
\[\log_2(64) = \log_2(2 \cdot 2 \cdot 2 \cdot 2 \cdot 2 \cdot 2) = \log_2(2^6) = 6 \cdot \log_2(2)=6\]</div>
<p>It is these properties we now use for likelihoods, using the letter
<span class="math notranslate nohighlight">\(p\)</span> instead of numbers. For example,</p>
<div class="math notranslate nohighlight">
\[\log_2(p^4) = 4 \log_2(p)\]</div>
<p>So when we have,</p>
<div class="math notranslate nohighlight">
\[l= p^4 \cdot (1-p)^6\]</div>
<p>we can take the logarithm to get</p>
<div class="math notranslate nohighlight">
\[L = \log_2 \left(p^4 \cdot (1-p)^6\right) = \log_2\left(p^4\right) + \log_2\left((1-p)^6\right) = 4 \log_2(p) + 6\log_2\left(1-p\right)\]</div>
<p>This is known as the <em>log-likelihood</em>. We can calulate the
log-likelihood for each value of <span class="math notranslate nohighlight">\(p\)</span> and plot <span class="math notranslate nohighlight">\(l\)</span> as a
function of <span class="math notranslate nohighlight">\(p\)</span>, as follows:</p>
<img alt="../_images/loglikelihood.png" src="../_images/loglikelihood.png" />
<p>Notice that it also has its maximum value at <span class="math notranslate nohighlight">\(p=0.4\)</span>, the value we
said was the maximum likelihood estimate of the proportion of people who
like gerkhins. If we find the value of <span class="math notranslate nohighlight">\(p\)</span> which maximises the
log-likelihood, we also find the value that maximises the likelihood.</p>
</div>
<div class="section" id="the-maximum-likelihood">
<h2>The Maximum Likelihood<a class="headerlink" href="#the-maximum-likelihood" title="Permalink to this heading"></a></h2>
<p>Let’s now prove that <span class="math notranslate nohighlight">\(p=0.4\)</span> is the maximum likelihood. Up to now
we have plotted the likelihood and log-likelihood, but we haven’t
demonstrated algebraically that the maximum must be <span class="math notranslate nohighlight">\(p=0.4\)</span>. To do
this we use differentiation. The derivative of a function tells its
slope at various points. For example, the function</p>
<div class="math notranslate nohighlight">
\[f(p) = p(1-p)\]</div>
<p>has derivative</p>
<div class="math notranslate nohighlight">
\[\frac{df}{dp} = 1 - 2p\]</div>
<p>We can find the maximum by solving</p>
<div class="math notranslate nohighlight">
\[\frac{df}{dp} = 1 - 2p = 0\]</div>
<p>which gives <span class="math notranslate nohighlight">\(p=1/2\)</span>. One reason we know that <span class="math notranslate nohighlight">\(p=1/2\)</span> is a
maximum (minimums of functions also have slope zero) is that the
derivative <span class="math notranslate nohighlight">\(\frac{dl}{dp}\)</span> is positive when <span class="math notranslate nohighlight">\(p&lt;1/2\)</span> and
negative when <span class="math notranslate nohighlight">\(p&gt;1/2\)</span>.</p>
<p>We can also take derivatives of functions containing logarithms. For
example, for the function</p>
<div class="math notranslate nohighlight">
\[F = \log_2(p) + \log_2(1-p)\]</div>
<p>the derivative is (<a class="reference external" href="https://www.khanacademy.org/math/ap-calculus-ab/ab-differentiation-2-new/ab-3-1b/v/logarithmic-functions-differentiation-intro">derivatives of logarithms are described
here</a>)</p>
<div class="math notranslate nohighlight">
\[\frac{dF}{dp} = \log_{\mbox{e}}(2) \left( \frac{1}{p} - \frac{1}{1-p} \right)\]</div>
<p>Again, this is zero when <span class="math notranslate nohighlight">\(p=1/2\)</span>, corresponding to the maximum.</p>
<p>We can now differentiate the likelihood function</p>
<div class="math notranslate nohighlight">
\[l(p) = p^4 \cdot (1-p)^6\]</div>
<p>Using the <a class="reference external" href="https://www.khanacademy.org/math/differential-calculus/dc-diff-intro#dc-product-rule">product rule for
derivatives</a>
we get</p>
<div class="math notranslate nohighlight">
\[\frac{dl}{dp} = -2p^3 (5p-2)(1-p)^5\]</div>
<p>Notice that derivatives has three solutions when set equal to zero:
<span class="math notranslate nohighlight">\(p=0\)</span>, <span class="math notranslate nohighlight">\(p=1\)</span> and <span class="math notranslate nohighlight">\(p=2/5\)</span>. We know that the likelihoods
at <span class="math notranslate nohighlight">\(p=0\)</span> and <span class="math notranslate nohighlight">\(p=1\)</span> are zero and thus minimima, so
<span class="math notranslate nohighlight">\(p=2/5\)</span> (i.e.<span class="math notranslate nohighlight">\(p=0.4\)</span>) is a maximum.</p>
<p>We get a similar result for the log-likelihood function</p>
<div class="math notranslate nohighlight">
\[L = 4 \log_2(p) + 6\log_2\left((1-p)\right)\]</div>
<p>the derivative is</p>
<div class="math notranslate nohighlight">
\[\frac{dL}{dp} = \log_{\mbox{e}}(2) \left( \frac{4}{p} - \frac{6}{1-p} \right)\]</div>
<p>This is zero when</p>
<div class="math notranslate nohighlight">
\[4(1-p) - 6p = 0\]</div>
<p>which, once again, occurs when <span class="math notranslate nohighlight">\(p=4/10\)</span>.</p>
<p>We have thus shown, both using the likelihood and the log-likelihood,
that the most likely estimate of the probability someone likes gherkins
is <span class="math notranslate nohighlight">\(p=4/10\)</span>.</p>
</div>
<div class="section" id="fishers-original-paper">
<h2>Fisher’s original paper<a class="headerlink" href="#fishers-original-paper" title="Permalink to this heading"></a></h2>
<p>Most people would accept that 0.4 as the correct answer to the gherkin
problem, even without the detailed maths I have done here. The point of
my presentation, though, is to introduce the way Fisher thought about
the problem of estimation using a problem we all understand. In 1912,
Fisher was trying to introduce a mathematical framework for evaluating
different ways of measuring in more complex situations as well. Now that
we have gone through the simpler problem, we can look at the article he
wrote over a hundred years ago.</p>
<p>Let’s start with the abstract:</p>
<img alt="../_images/FisherQuote1.png" src="../_images/FisherQuote1.png" />
<p>The problem Fisher is concerned with is the best way to fit a curve to
data. In our example above, we are estimating a single point (the
proportion of people who like gerkihns), but we might also be interested
in fitting a straight line through data points (as we do in the chapter
‘Happy world’) or maybe a quadratic curve to describe, for example, the
motion of a projectile.</p>
<p>Fisher rightly points out that there are “different standards of
conformity” between theory and data. He wants to know which one is
right. Or maybe it is more correct to say, that what he wants to find
out is the nature of the assumptions we make when we choose a particular
way of comparing a theoretical curve to data.</p>
<p>It is to this end he introduces the notion he will later call maximum
likelihood. He writes,</p>
<img alt="../_images/FisherQuote2.png" src="../_images/FisherQuote2.png" />
<p>Now, there are more mathematical symbols here than in our example. To
see the relationship look at the second of the two equations, which I
have rewritten slightly more explicit notation,</p>
<div class="math notranslate nohighlight">
\[\log(P) = \sum^{n}_{i=1} log(f(x_i))\]</div>
<p>Here <span class="math notranslate nohighlight">\(i\)</span> is an index over all <span class="math notranslate nohighlight">\(n\)</span> obsevrvations and
<span class="math notranslate nohighlight">\(f(x_i)\)</span> is the probability we see observation <span class="math notranslate nohighlight">\(x_i\)</span>. In our
gherkin example, <span class="math notranslate nohighlight">\(x_i\)</span> is zero when the person does not like the
gherkin and one when the person does like it. So,</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>:ma
th:`
x_1`</p></th>
<th class="head"><p>:ma
th:`
x_2`</p></th>
<th class="head"><p>:ma
th:`
x_3`</p></th>
<th class="head"><p>:ma
th:`
x_4`</p></th>
<th class="head"><p>:ma
th:`
x_5`</p></th>
<th class="head"><p>:ma
th:`
x_6`</p></th>
<th class="head"><p>:ma
th:`
x_7`</p></th>
<th class="head"><p>:ma
th:`
x_8`</p></th>
<th class="head"><p>:ma
th:`
x_9`</p></th>
<th class="head"><p>:m
ath:
<cite>x_{
10}</cite></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
</tbody>
</table>
<p>Fisher insists that we define a function <span class="math notranslate nohighlight">\(f(x)\)</span> which tells us the
probability we get a specific observation. In our case, we define the
function to be <span class="math notranslate nohighlight">\(f(0) = 1 - p\)</span> and <span class="math notranslate nohighlight">\(f(1) = p\)</span>, where
<span class="math notranslate nohighlight">\(p\)</span> is the parameter we want to estimate. Substituting,
<span class="math notranslate nohighlight">\(1-p\)</span> in to equation above whenever <span class="math notranslate nohighlight">\(x_i=0\)</span> and <span class="math notranslate nohighlight">\(p\)</span>
whenever <span class="math notranslate nohighlight">\(x_i=1\)</span> gives</p>
<div class="math notranslate nohighlight">
\[\log(P) = 4 log(p) + 6 log(1-p)\]</div>
<p>which is exactly the log-likelihood, which we called <span class="math notranslate nohighlight">\(L\)</span> and which
we saw above is maximised when <span class="math notranslate nohighlight">\(p=0.4\)</span>.</p>
<p>Fisher calls the parameter <span class="math notranslate nohighlight">\(\theta\)</span> instead of <span class="math notranslate nohighlight">\(p\)</span>. So we
could write <span class="math notranslate nohighlight">\(f(0) = 1 - \theta\)</span> and <span class="math notranslate nohighlight">\(f(1) = \theta\)</span> instead,
if we followed his notation. There is also a more subtle difference in
notation by Fisher, where he writes that the probability we see an
observation is <span class="math notranslate nohighlight">\(p = f(x) dx\)</span>, rather than <span class="math notranslate nohighlight">\(f\)</span> as I write
above. This is to allow us to model outcomes which have continuous
variables. In probability thoery <span class="math notranslate nohighlight">\(f(x)\)</span> is known as the
<a class="reference external" href="https://www.probabilitycourse.com/chapter4/4_1_1_pdf.php">probability density function
(pdf)</a> and
tells us probability per unit length (where <span class="math notranslate nohighlight">\(dx\)</span> is unit length)
as that length goes to zero. Specifically,</p>
<div class="math notranslate nohighlight">
\[f(x) = \lim_{dx \rightarrow 0} \frac{P(x &lt; X \leq x +dx)}{dx}\]</div>
<p>It is the probability that a value lies between <span class="math notranslate nohighlight">\(x\)</span> and
<span class="math notranslate nohighlight">\(x +dx\)</span> as <span class="math notranslate nohighlight">\(dx\)</span> gets very small.</p>
<p>The example of this which Fisher looks at the case wher the pdf as that
of the <a class="reference external" href="https://www.probabilitycourse.com/chapter4/4_2_3_normal.php">Normal
distribution</a>.
He writes</p>
<img alt="../_images/FisherQuote3.png" src="../_images/FisherQuote3.png" />
<p>Fisher thus shows that the Maximum Likelihood Estimate of the mean,
<span class="math notranslate nohighlight">\(m\)</span>, and variance, <span class="math notranslate nohighlight">\(s^2\)</span> of the distribution are</p>
<div class="math notranslate nohighlight">
\[m = \frac{1}{n} \sum^{n}_{i=1} x_i \mbox{ and } s^2 = \frac{1}{n} \sum^{n}_{i=1} (x_i-m)^2\]</div>
<p>(note that the variance is equal to <span class="math notranslate nohighlight">\(s^2=1/(2h^2)\)</span> in Fisher’s
notation). Fisher goes on to argue that the way that many other
statisticians estimate the variance, using</p>
<div class="math notranslate nohighlight">
\[s^2 = \frac{1}{n-1} \sum^{n}_{i=1} (x_i-m)^2\]</div>
<p>is wrong. In fact, statisticians now agree that it was Fisher who was
wrong and the <span class="math notranslate nohighlight">\(n-1\)</span> method, <a class="reference external" href="https://www.probabilitycourse.com/chapter8/8_2_2_point_estimators_for_mean_and_var.php">which is
unbiased</a>,
is most often used today. Fisher quietly <a class="reference external" href="https://projecteuclid.org/journals/statistical-science/volume-12/issue-3/RA-Fisher-and-the-making-of-maximum-likelihood-1912-1922/10.1214/ss/1030037906.pdf">changed his mind
too</a>
(although was loathe to explicitly admit it in writing).</p>
<p>What I do want to leave with you with is what I think is so fantastic
about the way Fisher worked as a student in 1912. For me, the paper
seems to come from a deep disatisfaction with just being told ‘this is
how you measure things’. He wanted to know why and, as a result, he
found a way of thinking about how lkely a particular set of data was.
Instead of just fitting a curve to data by measuring the distance
between points and the curve, he asked what is the probability I would
get this data, given my assumptions about the curve.</p>
</div>
</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="averagefriends.html" class="btn btn-neutral float-left" title="Very average friends" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../gallery/lesson1/plot_howtobehappy.html" class="btn btn-neutral float-right" title="Happy world" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, David Sumpter.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>